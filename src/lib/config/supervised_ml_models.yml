linear_regression:
    name: 'Linear Regression'
    description: 'Linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables)'
    hyperparameters:
        fit_intercept:
            value: true
            description: 'Specifies whether to calculate the intercept for this model'
    parameters:
        copy_X:
            value: true
            description: 'If True, X will be copied; else, it may be overwritten'
        positive:
            value: false
            description: 'Specifies whether to enforce positive coefficients'

ridge:
    name: 'Ridge Regression'
    description: 'Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients'
    hyperparameters:
        alpha:
            value: 1.0
            description: 'Constant that multiplies the L2 term, controlling regularization strength; must be a positive float [0, Inf].  For numerical reasons, using alpha = 0 with the Ridge object is not advised. Instead, you should use the LinearRegression.'
            fine_tune: [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]

    parameters:
        fit_intercept:
            value: true
            description: 'Specifies whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. X and y are expected to be centered).'
        normalize:
            value: false
            description: 'Specifies whether to normalize the input data'
        copy_X:
            value: true
            description: 'If True, X will be copied; else, it may be overwritten.'
        max_iter:
            value: null
            description: 'Maximum number of iterations for conjugate gradient solver'
        tol:
            value: 1e-4
            description: 'The precision of the solution (coef_) is determined by tol which specifies a different convergence criterion for each solver'
        positive:
            value: false
            description: 'Specifies whether to enforce positive coefficients'
        solver:
            value:
                default: 'auto'
                options:
                    auto: 'Automatic selection of the most appropriate solver for the data'
                    svd: 'Singular Value Decomposition'
                    cholesky: 'Cholesky decomposition'
                    lsqr: 'Least Squares solution'
                    sparse_cg: 'Conjugate Gradient solver'
                    sag: 'Stochastic Average Gradient descent'
                    saga: 'SAGA solver'
            description: 'Solver to use in the computational routines'

svr:
    name: 'Support Vector Regression'
    description: 'Support Vector Regression (SVR) uses the same principles as the SVM for classification, with only a few minor differences'
    hyperparameters:
        kernel:
            value:
                default: 'rbf'
                options:
                    linear: 'Linear kernel'
                    poly: 'Polynomial kernel'
                    rbf: 'Radial Basis Function kernel'
                    sigmoid: 'Sigmoid kernel'
                    precomputed: 'Precomputed kernel'
            description: 'Specifies the kernel type to be used in the algorithm'
            fine_tune: ['linear', 'poly', 'rbf', 'sigmoid']
        gamma:
            value:
                default: 'scale'
                options:
                    scale: '1 / (n_features * X.var())'
                    auto: '1 / n_features'
                    float: 'must be non-negative'
            description: "Kernel coefficient for 'rbf', 'poly' and 'sigmoid'."
            fine_tune: ['scale', 'auto', 0.001, 0.01, 0.1, 1, 10, 100, 1000]
        C:
            value: 1.0
            description: 'Regularization parameter'
            fine_tune: [0.001, 0.01, 0.1, 1, 10, 100, 1000]
        epsilon:
            value: 0.1
            description: 'Epsilon in the epsilon-SVR model'
            fine_tune: [0.01, 0.1, 0.5, 1, 2, 5, 10]
    parameters:
        degree:
            value: 3
            description: 'Degree of the polynomial kernel function (only for poly). Must be non-negative'

        coef0:
            value: 0.0
            description: 'Independent term in kernel function. It is only significant in poly and sigmoid.'
        tol:
            value: 1e-3
            description: 'Tolerance for stopping criteria'

        shrinking:
            value: true
            description: 'Specifies whether to use the shrinking heuristic'
        cache_size:
            value: 200
            description: 'Size of the kernel cache in MB'
        verbose:
            value: false
            description: 'Specifies whether to enable verbose output'
        max_iter:
            value: -1
            description: 'Hard limit on iterations within solver, or -1 for no limit'

knn:
    name: 'K-Nearest Neighbors'
    description: 'K-Nearest Neighbors (KNN) is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until function evaluation'
    hyperparameters:
        n_neighbors:
            value: 5
            description: 'Number of neighbors to use for kneighbors queries'
        weights:
            value:
                default: 'uniform'
                options:
                    uniform: 'All points in each neighborhood are weighted equally'
                    distance: 'Weight points by the inverse of their distance'
            description: 'Weight function used in prediction'
        metric:
            value:
                default: 'minkowski'
                options:
                    minkowski: 'Minkowski distance'
                    cosine: 'Cosine distance'
                    euclidean: 'Euclidean distance'
                    nan_euclidean: 'Euclidean distance ignoring NaN values'
                    manhattan: 'Manhattan distance'
            description: 'Metric to use for distance computation'
            fine_tune: ['minkowski', 'cosine', 'euclidean', 'nan_euclidean', 'manhattan']
    parameters:
        algorithm:
            value:
                default: 'auto'
                options:
                    auto: 'Automatically select the most appropriate algorithm'
                    ball_tree: 'BallTree algorithm'
                    kd_tree: 'KDTree algorithm'
                    brute: 'Brute-force search algorithm'
            description: 'Algorithm used to compute the nearest neighbors'
        leaf_size:
            value: 30
            description: 'Leaf size passed to BallTree or KDTree'
        p:
            value: 2
            description: 'Power parameter for the Minkowski metric'
        metric_params:
            value: null
            description: 'Additional keyword arguments for the metric function'

rfr:
    name: 'Random Forest Regression'
    description: 'Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees'
    hyperparameters:
        n_estimators:
            value: 100
            description: 'Number of trees in the forest'
            fine_tune: [10, 50, 100, 200, 500]
        max_leaf_nodes:
            value: null
            description: 'Grow trees with max_leaf_nodes in best-first fashion'
            fine_tune: [null, 5, 10, 20, 50, 100]
        min_samples_leaf:
            value: 1
            description: 'Minimum number of samples required to be at a leaf node'
            fine_tune: [0.1, 0.3, 0.5, 0.7, 1]
        max_depth:
            value: null
            description: 'Maximum depth of the tree'
            fine_tune: [5, 10, 15, 20, 25]
    parameters:
        criterion:
            value:
                default: 'squared_error'
                options:
                    squared_error: 'Mean squared error'
                    absolute_error: 'Mean absolute error'
                    friedman_mse: 'Mean squared error with improvement score by Friedman'
                    poisson: 'Mean squared error for Poisson distribution'
            description: 'Function to measure the quality of a split'
        min_samples_split:
            value: 2
            description: 'Minimum number of samples required to split an internal node'
        min_weight_fraction_leaf:
            value: 0.0
            description: 'Minimum weighted fraction of the sum total of weights required to be at a leaf node'
        max_features:
            value:
                default: null
                options:
                    log2: 'log2(n_features)'
                    sqrt: 'sqrt(n_features)'
                    null: 'n_features'
                    float: 'n_features'
            description: 'Number of features to consider when looking for the best split'
        min_impurity_decrease:
            value: 0.0
            description: 'A node will be split if this split induces a decrease of the impurity greater than or equal to this value'
        min_impurity_split:
            value: null
            description: 'Threshold for early stopping in tree growth'
        bootstrap:
            value: true
            description: 'Whether bootstrap samples are used when building trees'
        oob_score:
            value: false
            description: 'Whether to use out-of-bag samples to estimate the R^2 on unseen data'
        verbose:
            value: 0
            description: 'Controls the verbosity when fitting and predicting'
        warm_start:
            value: false
            description: 'When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble'
        ccp_alpha:
            value: 0.0
            description: 'Complexity parameter used for Minimal Cost-Complexity Pruning'
        max_samples:
            value: null
            description: 'If bootstrap is True, the number of samples to draw from X to train each base estimator'
        monotonic_cst:
            value: null
            description: 'List of constraints to impose monotonicity on features. 1: monotonically increasing, 0: no constraint, -1: monotonically decreasing'

gbr:
    name: 'Gradient Boosting Regression'
    description: 'Gradient Boosting builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions'
    hyperparameters:
        learning_rate:
            value: 0.1
            description: 'Learning rate shrinks the contribution of each tree'
            fine_tune: [0.001, 0.01, 0.1, 0.5, 1]
        n_estimators:
            value: 100
            description: 'The number of boosting stages to perform'
            fine_tune: [10, 50, 100, 200, 500]
        min_samples_split:
            value: 2
            description: 'Minimum number of samples required to split an internal node'
            fine_tune: [0.1, 0.3, 0.5, 0.7, 1]
        min_samples_leaf:
            value: 1
            description: 'Minimum number of samples required to be at a leaf node'
            fine_tune: [0.1, 0.3, 0.5, 0.7, 1]
        subsample:
            value: 1.0
            description: 'The fraction of samples to be used for fitting the individual base learners'
            fine_tune: [0.1, 0.3, 0.5, 0.7, 1]
        max_depth:
            value: 3
            description: 'Maximum depth of the individual regression estimators'
            fine_tune: [3, 5, 7, 9, 11]
    parameters:
        loss:
            value:
                default: 'squared_error'
                options:
                    squared_error: 'Least squares regression'
                    absolute_error: 'Least absolute deviation regression'
                    huber: 'Huber loss for robust regression'
                    quantile: 'Quantile regression'
            description: 'Loss function to be optimized'
        criterion:
            value:
                default: 'friedman_mse'
                options:
                    friedman_mse: 'Mean squared error with improvement score by Friedman'
                    squared_error: 'Mean squared error'
            description: 'Function to measure the quality of a split'
        min_weight_fraction_leaf:
            value: 0.0
            description: 'Minimum weighted fraction of the sum total of weights required to be at a leaf node'
        min_impurity_decrease:
            value: 0.0
            description: 'A node will be split if this split induces a decrease of the impurity greater than or equal to this value'
        init:
            value: null
            description: 'An estimator object that is used to compute the initial predictions'
        max_features:
            value:
                default: null
                options:
                    log2: 'log2(n_features)'
                    sqrt: 'sqrt(n_features)'
                    null: 'n_features'
                    float: 'n_features'
            description: 'Number of features to consider when looking for the best split'
        alpha:
            value: 0.9
            description: 'The alpha-quantile of the huber loss function and the quantile loss function'
        verbose:
            value: 0
            description: 'Controls the verbosity when fitting and predicting'
        max_leaf_nodes:
            value: null
            description: 'Grow trees with max_leaf_nodes in best-first fashion'
        warm_start:
            value: false
            description: 'When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble'
        validation_fraction:
            value: 0.1
            description: 'The proportion of training data to set aside as validation set for early stopping'
        n_iter_no_change:
            value: null
            description: 'Number of iterations with no improvement to wait before early stopping'
        tol:
            value: 1e-4
            description: 'Tolerance for the early stopping criterion'
        ccp_alpha:
            value: 0.0
            description: 'Complexity parameter used for Minimal Cost-Complexity Pruning'

gpr:
    name: 'Gaussian Process Regression'
    description: 'Gaussian processes are a powerful, non-parametric approach to regression'
    hyperparameters:
        alpha:
            value: 1e-10
            description: 'Value added to the diagonal of the kernel matrix during fitting'
            fine_tune: [1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]
        n_restarts_optimizer:
            value: 0
            description: 'The number of restarts of the optimizer for finding the kernel parameters'
            fine_tune: [0, 1, 2, 3, 4, 5]
    parameters:
        kernel:
            value: null
            description: 'Kernel specifying the covariance function of the GP'
        optimizer:
            value: 'fmin_l_bfgs_b'
            description: 'The optimizer to use for optimizing the kernel parameters'
        normalize_y:
            value: false
            description: 'Whether to normalize the target values y by removing the mean and scaling to unit-variance. This is recommended for cases where zero-mean, unit-variance priors are used.'
        copy_X_train:
            value: true
            description: 'If True, a persistent copy of the training data is stored in the object. Otherwise, just a reference to the training data is stored, which might cause predictions to change if the data is modified externally.'
        n_targets:
            value: null
            description: 'The number of dimensions of the target values. Used to decide the number of outputs when sampling from the prior distributions (i.e. calling sample_y before fit). This parameter is ignored once fit has been called'
